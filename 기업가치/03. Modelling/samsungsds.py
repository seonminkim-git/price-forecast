# -*- coding: utf-8 -*-
"""samsungSDS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wVS4RbYb9-AMKoa2scM-KyHUGe0BmNmU
"""

import warnings
warnings.filterwarnings('ignore')

# 데이터 읽기를 위한 라이브러리
import numpy as np
np.random.seed(0)
import pandas as pd
import scipy as sp
import gc, os, time
import statsmodels.api as sm
from pandas import DataFrame, Series
from datetime import datetime, date, timedelta
from sklearn.preprocessing import StandardScaler, LabelEncoder

# 탐색적 데이터 분석을 위한 라이브러리
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew, norm, probplot, boxcox

# 모델링을 위한 라이브러리
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.feature_selection import SelectKBest, mutual_info_regression, VarianceThreshold
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor

# 하이퍼파라미터 튜닝 
from functools import partial
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

from google.colab import drive
drive.mount('/content/drive')

# 데이터 로드 부분입니다  
path = 'drive/MyDrive/주가데이터분석/data/'
df_raw = pd.read_csv(f'{path}samsungSDS.csv', index_col=0)
df_raw.columns = df_raw.columns.str.lower()

# 평가지표 함수 
def mape(y_true, y_pred):
  return round(mean_absolute_percentage_error(y_true, y_pred), 4)

# 모델링을 위한 변수 정의 
target = 'y'
test_period = -20 # 향후 20점을 예측함
col_X = df_raw.iloc[:, 0:-4].columns.tolist() # X 인자

df_raw.tail()

pred_te_rf = []
pred_te_gbm = []
pred_te_xgb = []

y_true = df_raw.iloc[test_period:,:][target].values

# 학습 데이터를 늘려가면서 진행 
for ix in range(test_period, 0):

  df_train = df_raw.iloc[:ix, :] 
  df_test = df_raw.iloc[ix:, :].head(1)

  x_train = df_train[col_X]
  x_test = df_test[col_X]

  y_train = df_train[target]
  
  # 랜덤포레스트 
  reg_model = RandomForestRegressor(n_estimators=500, criterion='mse', max_depth=None, max_leaf_nodes=10, min_samples_split=2, min_samples_leaf=5, max_features=0.3, bootstrap=False)
  reg_model.fit(x_train, y_train)
  y_pred = reg_model.predict(x_test)
  pred_te_rf.append(y_pred[0])

  # 그라디언트 부스팅 
  reg_model = GradientBoostingRegressor(learning_rate=0.005, n_estimators=500, max_depth=3, max_leaf_nodes=None, min_samples_split=2, min_samples_leaf=1, subsample=1.0, max_features=0.3, alpha=0.9, )
  reg_model.fit(x_train, y_train)
  y_pred = reg_model.predict(x_test)
  pred_te_gbm.append(y_pred[0])

  # XGB
  reg_model = XGBRegressor(random_state=0, objective='reg:squarederror', subsample=0.8, max_depth=5, reg_alpha=0.1, colsample_bytree=1.0, colsample_bylevel=1.0, min_child_weight=3, gamma=0.0)
  reg_model.fit(x_train, y_train)
  y_pred = reg_model.predict(x_test)
  pred_te_xgb.append(y_pred[0])

print(f'RF Test Loss: {mape(y_true, pred_te_rf)}')
print(f'GBM Test Loss: {mape(y_true, pred_te_gbm)}')
print(f'XGB Test Loss: {mape(y_true, pred_te_xgb)}')

df_recap_rf = pd.DataFrame({'y_true': y_true, 'y_pred': pred_te_rf})
df_recap_gbm = pd.DataFrame({'y_true': y_true, 'y_pred': pred_te_gbm})
df_recap_xgb = pd.DataFrame({'y_true': y_true, 'y_pred': pred_te_xgb})

# 시각화 
fig, axes = plt.subplots(nrows=1, ncols=3, sharey=False, figsize=(18,4))
fig.subplots_adjust(hspace=.4, wspace=.3)

sns.lineplot(data=df_recap_rf, ax=axes[0])
sns.lineplot(data=df_recap_gbm, ax=axes[1])
sns.lineplot(data=df_recap_xgb, ax=axes[2])

axes[0].set_title('RF')
axes[1].set_title('GBM')
axes[2].set_title('XGB')

plt.show()
plt.clf()